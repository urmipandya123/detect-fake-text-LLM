{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":61542,"databundleVersionId":6888007,"sourceType":"competition"},{"sourceId":6300093,"sourceType":"datasetVersion","datasetId":3623988},{"sourceId":6857742,"sourceType":"datasetVersion","datasetId":3623154},{"sourceId":6987454,"sourceType":"datasetVersion","datasetId":3947266},{"sourceId":151103719,"sourceType":"kernelVersion"}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install /kaggle/input/llm-science-exam-lib-ds/keras_core-0.1.7-py3-none-any.whl --no-deps\n!pip install /kaggle/input/llm-science-exam-lib-ds/keras_nlp-0.6.2-py3-none-any.whl --no-deps","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-01T10:03:52.133730Z","iopub.execute_input":"2024-01-01T10:03:52.134009Z","iopub.status.idle":"2024-01-01T10:03:57.915111Z","shell.execute_reply.started":"2024-01-01T10:03:52.133984Z","shell.execute_reply":"2024-01-01T10:03:57.913983Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Processing /kaggle/input/llm-science-exam-lib-ds/keras_core-0.1.7-py3-none-any.whl\nkeras-core is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\nProcessing /kaggle/input/llm-science-exam-lib-ds/keras_nlp-0.6.2-py3-none-any.whl\nInstalling collected packages: keras-nlp\n  Attempting uninstall: keras-nlp\n    Found existing installation: keras-nlp 0.6.3\n    Uninstalling keras-nlp-0.6.3:\n      Successfully uninstalled keras-nlp-0.6.3\nSuccessfully installed keras-nlp-0.6.2\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nos.environ[\"KERAS_BACKEND\"] = \"torch\"  # or \"tensorflow\" or \"torch\"\n\nimport keras_nlp\nimport keras_core as keras \nimport keras_core.backend as K\n\n\nimport jax\nimport tensorflow as tf\n# from tensorflow import keras\n# import tensorflow.keras.backend as K\n\nimport numpy as np \nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n\nfrom glob import glob\nfrom tqdm.notebook import tqdm\nimport gc","metadata":{"execution":{"iopub.status.busy":"2024-01-01T10:04:02.018714Z","iopub.execute_input":"2024-01-01T10:04:02.019096Z","iopub.status.idle":"2024-01-01T10:04:19.350669Z","shell.execute_reply.started":"2024-01-01T10:04:02.019054Z","shell.execute_reply":"2024-01-01T10:04:19.349687Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Using PyTorch backend.\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"print(\"TensorFlow:\", tf.__version__)\n# print(\"JAX:\", jax.__version__)\nprint(\"Keras:\", keras.__version__)\nprint(\"KerasNLP:\", keras_nlp.__version__)","metadata":{"execution":{"iopub.status.busy":"2024-01-01T10:04:27.015876Z","iopub.execute_input":"2024-01-01T10:04:27.016816Z","iopub.status.idle":"2024-01-01T10:04:27.021576Z","shell.execute_reply.started":"2024-01-01T10:04:27.016781Z","shell.execute_reply":"2024-01-01T10:04:27.020650Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"TensorFlow: 2.13.0\nKeras: 0.1.7\nKerasNLP: 0.6.2\n","output_type":"stream"}]},{"cell_type":"code","source":"class CFG:\n    verbose = 0  # Verbosity\n    device = 'GPU'  # Device\n    seed = 42  # Random seed\n    batch_size = 6  # Batch size\n    drop_remainder = True  # Drop incomplete batches\n    ckpt_dir = \"/kaggle/input/daigt-kerasnlp-ckpt\"  # Name of pretrained models\n    sequence_length = 200  # Input sequence length\n    class_names = ['real','fake']  # Class names [A, B, C, D, E]\n    num_classes = len(class_names)  # Number of classes\n    class_labels = list(range(num_classes))  # Class labels [0, 1, 2, 3, 4]\n    label2name = dict(zip(class_labels, class_names))  # Label to class name mapping\n    name2label = {v: k for k, v in label2name.items()}  # Class name to label mapping","metadata":{"execution":{"iopub.status.busy":"2024-01-01T10:04:29.036988Z","iopub.execute_input":"2024-01-01T10:04:29.037334Z","iopub.status.idle":"2024-01-01T10:04:29.043559Z","shell.execute_reply.started":"2024-01-01T10:04:29.037308Z","shell.execute_reply":"2024-01-01T10:04:29.042542Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"keras.utils.set_random_seed(CFG.seed)","metadata":{"execution":{"iopub.status.busy":"2024-01-01T10:04:30.374838Z","iopub.execute_input":"2024-01-01T10:04:30.375422Z","iopub.status.idle":"2024-01-01T10:04:30.387409Z","shell.execute_reply.started":"2024-01-01T10:04:30.375383Z","shell.execute_reply":"2024-01-01T10:04:30.386017Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def get_device():\n    \"Detect and intializes GPU/TPU automatically\"\n    try:\n        # Connect to TPU\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect() \n        # Set TPU strategy\n        strategy = tf.distribute.TPUStrategy(tpu)\n        print(f'> Running on TPU', tpu.master(), end=' | ')\n        print('Num of TPUs: ', strategy.num_replicas_in_sync)\n        device=CFG.device\n    except:\n        # If TPU is not available, detect GPUs\n        gpus = tf.config.list_logical_devices('GPU')\n        ngpu = len(gpus)\n         # Check number of GPUs\n        if ngpu:\n            # Set GPU strategy\n            strategy = tf.distribute.MirroredStrategy(gpus) # single-GPU or multi-GPU\n            # Print GPU details\n            print(\"> Running on GPU\", end=' | ')\n            print(\"Num of GPUs: \", ngpu)\n            device='GPU'\n        else:\n            # If no GPUs are available, use CPU\n            print(\"> Running on CPU\")\n            strategy = tf.distribute.get_strategy()\n            device='CPU'\n    return strategy, device","metadata":{"execution":{"iopub.status.busy":"2024-01-01T10:04:34.571399Z","iopub.execute_input":"2024-01-01T10:04:34.572143Z","iopub.status.idle":"2024-01-01T10:04:34.579657Z","shell.execute_reply.started":"2024-01-01T10:04:34.572110Z","shell.execute_reply":"2024-01-01T10:04:34.578690Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Initialize GPU/TPU/TPU-VM\nstrategy, CFG.device = get_device()\nCFG.replicas = strategy.num_replicas_in_sync","metadata":{"execution":{"iopub.status.busy":"2024-01-01T10:04:37.660339Z","iopub.execute_input":"2024-01-01T10:04:37.660672Z","iopub.status.idle":"2024-01-01T10:04:45.159239Z","shell.execute_reply.started":"2024-01-01T10:04:37.660648Z","shell.execute_reply":"2024-01-01T10:04:45.158218Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"> Running on GPU | Num of GPUs:  2\n","output_type":"stream"}]},{"cell_type":"code","source":"BASE_PATH = '/kaggle/input/llm-detect-ai-generated-text'","metadata":{"execution":{"iopub.status.busy":"2024-01-01T10:04:45.161009Z","iopub.execute_input":"2024-01-01T10:04:45.161339Z","iopub.status.idle":"2024-01-01T10:04:45.172698Z","shell.execute_reply.started":"2024-01-01T10:04:45.161312Z","shell.execute_reply":"2024-01-01T10:04:45.171960Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv(f'{BASE_PATH}/test_essays.csv')  # Read CSV file into a DataFrame\n\n# Display information about the train data\nprint(\"# Test Data: {:,}\".format(len(test_df)))\nprint(\"# Sample:\")\ndisplay(test_df.head(2))","metadata":{"execution":{"iopub.status.busy":"2024-01-01T10:04:47.085072Z","iopub.execute_input":"2024-01-01T10:04:47.085406Z","iopub.status.idle":"2024-01-01T10:04:47.119854Z","shell.execute_reply.started":"2024-01-01T10:04:47.085383Z","shell.execute_reply":"2024-01-01T10:04:47.118986Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"# Test Data: 3\n# Sample:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"         id  prompt_id          text\n0  0000aaaa          2  Aaa bbb ccc.\n1  1111bbbb          3  Bbb ccc ddd.","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>prompt_id</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000aaaa</td>\n      <td>2</td>\n      <td>Aaa bbb ccc.</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1111bbbb</td>\n      <td>3</td>\n      <td>Bbb ccc ddd.</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"vocab_path = '/kaggle/input/keras-nlp-deberta-v3-base-en-vocab-ds/vocab.spm'\ntokenizer= keras_nlp.models.DebertaV3Tokenizer(vocab_path)\npreprocessor= keras_nlp.models.DebertaV3Preprocessor(tokenizer, sequence_length=CFG.sequence_length)","metadata":{"execution":{"iopub.status.busy":"2024-01-01T10:04:47.820009Z","iopub.execute_input":"2024-01-01T10:04:47.820365Z","iopub.status.idle":"2024-01-01T10:04:48.540419Z","shell.execute_reply.started":"2024-01-01T10:04:47.820338Z","shell.execute_reply":"2024-01-01T10:04:48.539657Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"outs = preprocessor(test_df.text.iloc[0])  # Process options for the first row\n\n# Display the shape of each processed output\nfor k, v in outs.items():\n    print(k, \":\", v.shape)","metadata":{"execution":{"iopub.status.busy":"2024-01-01T10:04:49.868508Z","iopub.execute_input":"2024-01-01T10:04:49.868855Z","iopub.status.idle":"2024-01-01T10:04:50.027826Z","shell.execute_reply.started":"2024-01-01T10:04:49.868826Z","shell.execute_reply":"2024-01-01T10:04:50.026802Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"token_ids : torch.Size([200])\npadding_mask : torch.Size([200])\n","output_type":"stream"}]},{"cell_type":"code","source":"def preprocess_fn(text, label=None):\n    text = preprocessor(text)  # Preprocess text\n    return (text, label) if label is not None else text  # Return processed text and label if available","metadata":{"execution":{"iopub.status.busy":"2024-01-01T10:04:51.080407Z","iopub.execute_input":"2024-01-01T10:04:51.080872Z","iopub.status.idle":"2024-01-01T10:04:51.086009Z","shell.execute_reply.started":"2024-01-01T10:04:51.080842Z","shell.execute_reply":"2024-01-01T10:04:51.084969Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def build_dataset(texts, labels=None, batch_size=32,\n                  cache=False, drop_remainder=True,\n                  augment=False, repeat=False, shuffle=1024):\n    AUTO = tf.data.AUTOTUNE  # AUTOTUNE option\n    slices = (texts,) if labels is None else (texts, labels)  # Create slices\n    ds = tf.data.Dataset.from_tensor_slices(slices)  # Create dataset from slices\n    ds = ds.cache() if cache else ds  # Cache dataset if enabled\n    ds = ds.map(preprocess_fn, num_parallel_calls=AUTO)  # Map preprocessing function\n    ds = ds.repeat() if repeat else ds  # Repeat dataset if enabled\n    opt = tf.data.Options()  # Create dataset options\n    if shuffle: \n        ds = ds.shuffle(shuffle, seed=CFG.seed)  # Shuffle dataset if enabled\n        opt.experimental_deterministic = False\n    ds = ds.with_options(opt)  # Set dataset options\n    ds = ds.batch(batch_size, drop_remainder=drop_remainder)  # Batch dataset\n    ds = ds.prefetch(AUTO)  # Prefetch next batch\n    return ds  # Return the built dataset","metadata":{"execution":{"iopub.status.busy":"2024-01-01T10:04:52.641441Z","iopub.execute_input":"2024-01-01T10:04:52.642199Z","iopub.status.idle":"2024-01-01T10:04:52.649739Z","shell.execute_reply.started":"2024-01-01T10:04:52.642166Z","shell.execute_reply":"2024-01-01T10:04:52.648763Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def get_test_dataset(test_df):\n    test_texts = test_df.text.tolist()  # Extract testation texts\n    \n    # Build testation dataset\n    test_ds = build_dataset(test_texts, labels=None,\n                             batch_size=min(CFG.batch_size*CFG.replicas, len(test_df)), cache=False,\n                             shuffle=False, drop_remainder=False, repeat=False)\n    \n    return test_ds  # Return datasets and dataframes","metadata":{"execution":{"iopub.status.busy":"2024-01-01T10:04:57.321340Z","iopub.execute_input":"2024-01-01T10:04:57.321992Z","iopub.status.idle":"2024-01-01T10:04:57.328023Z","shell.execute_reply.started":"2024-01-01T10:04:57.321953Z","shell.execute_reply":"2024-01-01T10:04:57.326962Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def build_model():\n    # Create a DebertaV3Classifier model\n    classifier = keras_nlp.models.DebertaV3Classifier.from_preset(\n        CFG.preset,\n        load_weights=False,\n        preprocessor=None,\n        num_classes=1 # one output per one option, for five options total 5 outputs\n    )\n    inputs = classifier.input\n    logits = classifier(inputs)\n        \n    # Compute final output\n    outputs = keras.layers.Activation(\"sigmoid\")(logits)\n    model = keras.Model(inputs, outputs)\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-01-01T10:04:58.515209Z","iopub.execute_input":"2024-01-01T10:04:58.515561Z","iopub.status.idle":"2024-01-01T10:04:58.521696Z","shell.execute_reply.started":"2024-01-01T10:04:58.515527Z","shell.execute_reply":"2024-01-01T10:04:58.520623Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# Get the checkpoint directory and name\nckpt_dir = CFG.ckpt_dir\nckpt_name = ckpt_dir.split('/')[3]\n\n# Copy the checkpoints to a new directory in the /kaggle directory\n!cp -r {ckpt_dir} /kaggle/{ckpt_name}\n\n# List all the checkpoint paths in the new directory\nnew_ckpt_dir = f\"/kaggle/{ckpt_name}\"\nckpt_paths = glob(os.path.join(new_ckpt_dir, '*.keras'))\n\nprint(\"Total CKPT:\", len(ckpt_paths))","metadata":{"execution":{"iopub.status.busy":"2024-01-01T10:04:59.739654Z","iopub.execute_input":"2024-01-01T10:04:59.740492Z","iopub.status.idle":"2024-01-01T10:05:44.137795Z","shell.execute_reply.started":"2024-01-01T10:04:59.740461Z","shell.execute_reply":"2024-01-01T10:05:44.136559Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Total CKPT: 3\n","output_type":"stream"}]},{"cell_type":"code","source":"# Initialize an array to store predictions for each fold\nfold_preds = np.zeros(shape=(len(test_df),), dtype='float32')\n\n# # Build model\n# model = build_model()\n\n# Iterate through each checkpoint path\nfor ckpt_path in tqdm(ckpt_paths):\n    # Load the pre-trained model from the checkpoint\n    model = keras.models.load_model(\n        ckpt_path,\n        compile=False,\n    )\n#     model.load_weights(ckpt_path)\n    \n    # Get the test dataset\n    test_ds = get_test_dataset(test_df)\n    \n    # Generate predictions using the model\n    preds = model.predict(\n        test_ds,\n        batch_size=min(CFG.batch_size * CFG.replicas * 2, len(test_df)),  # Set batch size\n        verbose=1\n    )\n    \n    # Add predictions to fold_preds and average over checkpoints\n    fold_preds += preds.squeeze() / len(ckpt_paths)\n    \n    # Clean up by deleting the model and collecting garbage\n    del model\n    gc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-01-01T10:05:56.276640Z","iopub.execute_input":"2024-01-01T10:05:56.277132Z","iopub.status.idle":"2024-01-01T10:13:32.819251Z","shell.execute_reply.started":"2024-01-01T10:05:56.277098Z","shell.execute_reply":"2024-01-01T10:13:32.818302Z"},"trusted":true},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e4d70f419254ed799faea9674992334"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras_core/src/trainers/trainer.py:166: UserWarning: `jit_compile` is not yet enabled for the PyTorch backend. Proceeding with `jit_compile=False`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/keras_core/src/saving/serialization_lib.py:713: UserWarning: `compile()` was not called as part of model loading because the model's `compile()` method is custom. All subclassed Models that have `compile()` overridden should also override `get_compile_config()` and `compile_from_config(config)`. Alternatively, you can call `compile()` manually after loading.\n  instance.compile_from_config(compile_config)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 291ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 310ms/step\n","output_type":"stream"}]},{"cell_type":"code","source":"# Format predictions and true answers\npred_answers = (fold_preds > 0.5).astype(int).squeeze()\n\n# Check 5 Predictions\nprint(\"# Predictions\\n\")\nfor i in range(3):\n    row = test_df.iloc[i]\n    text  = row.text\n    pred_answer = CFG.label2name[pred_answers[i]]\n    print(f\"‚ùì Text {i+1}:\\n{text}\\n\")\n    print(f\"ü§ñ Predicted: {pred_answer}\\n\")\n    print(\"-\"*90, \"\\n\")","metadata":{"execution":{"iopub.status.busy":"2024-01-01T10:14:08.874644Z","iopub.execute_input":"2024-01-01T10:14:08.875537Z","iopub.status.idle":"2024-01-01T10:14:08.884719Z","shell.execute_reply.started":"2024-01-01T10:14:08.875482Z","shell.execute_reply":"2024-01-01T10:14:08.883488Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"# Predictions\n\n‚ùì Text 1:\nAaa bbb ccc.\n\nü§ñ Predicted: fake\n\n------------------------------------------------------------------------------------------ \n\n‚ùì Text 2:\nBbb ccc ddd.\n\nü§ñ Predicted: fake\n\n------------------------------------------------------------------------------------------ \n\n‚ùì Text 3:\nCCC ddd eee.\n\nü§ñ Predicted: fake\n\n------------------------------------------------------------------------------------------ \n\n","output_type":"stream"}]},{"cell_type":"code","source":"# Create a DataFrame to store the submission\nsub_df = test_df[[\"id\"]].copy()\n\n# Add the formatted predictions to the submission DataFrame\nsub_df[\"generated\"] = fold_preds.squeeze()\n\n# Save Submission\nsub_df.to_csv('submission.csv',index=False)\n\n# Display the first 2 rows of the submission DataFrame\nsub_df.head(2)","metadata":{"execution":{"iopub.status.busy":"2024-01-01T10:14:10.328137Z","iopub.execute_input":"2024-01-01T10:14:10.328496Z","iopub.status.idle":"2024-01-01T10:14:10.353480Z","shell.execute_reply.started":"2024-01-01T10:14:10.328466Z","shell.execute_reply":"2024-01-01T10:14:10.352622Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"         id  generated\n0  0000aaaa   0.598033\n1  1111bbbb   0.618186","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>generated</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0000aaaa</td>\n      <td>0.598033</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1111bbbb</td>\n      <td>0.618186</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}